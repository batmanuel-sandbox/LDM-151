\section{Alert Production}
\label{sec:ap}



Alert Production is run each night to product catalogs and images for sources that have varied or moved relative to a previous observation.  The data products produced by Alert production are given in  table~\hyperref[table:ap_data_products].

\begin{table}
\small
\begin{tabularx}{\textwidth}{ | l | l | X | }
  \hline
  {\bf Name} & {\bf Availability} & {\bf Description} \\
  \hline
  DIASource & Stored &
  Measurements from difference image analysis of individual exposures. \\
  \hline
  DIAObject& Stored &
  Aggregate quantities computed by associating spatially colocated DIASources. \\
  \hline
  DIAForcedSource & Stored &
  Flux measurements on each difference image at the position of every DIAObject. \\
  \hline
  SSObject & Stored &
  Solar system objects derived by associating DIASources and inferring their orbits. \\
  \hline
  CalExp & Stored &
  Calibrated exposure images for each CCD/visit (sum of two snaps) and associated metadata (e.g.\ WCS and estimated background). \\
  \hline
  DiffExp & Stored &
  Difference between CalExp and PSF-matched template coadd. \\
  \hline
\end{tabularx}
\caption{Table of derived and persisted data products produced during  Alert Production.  A description of these data products can be found in the Data Products Definition Document (LSE-163).
\label{table:ap_data_products}}
\end{table}


Alert Production is designed as five separate components: single frame processing, alert detection, alert generation, precovery photometry, and a moving objects pipeline. The first four of these components run as a linear pass through of the data. The moving objects pipeline is run independently of the rest of the alert production. The flow of information through this system is shown in figure~\ref{fig:nightly}.

\begin{figure}[th]
\begin{center}
\includegraphics[width=0.9\textwidth]{figures/Level_1_Processing_Flowchart.jpg}
\caption{\label{fig:nightly} The alert production flow of data through the processing pipelines (single frame processing, alert detection,  alert generation, precovery photometry) }
\end{center}
\end{figure}

In this document we do not address the question of estimating the selection function for the alert generation through the injection of simulated sources. Such a process could be undertaken in a parallel processing string that starts from the generation of the CalExp images. Given the available computational resources  this would likely only be able to sample the selection function over a subset of the data.

\subsection{Single Frame Processing Pipeline (\wbsSFM)}
\label{sec:apSingleFrameProcessing}

Single Frame Processing (SFM) Pipeline is responsible for reducing raw or camera corrected image data to\emph{calibrated exposures}, the detection and measurement of \Sources (using the components functionally  part of the Object Characterization Pipeline), the characterization of the point-spread-function (PSF), and the generation of an astrometric solution for an image.

Detection and measurement of the properties of \Sources used to astrometrically and photometrically calibrate the CCD images requires knowlege of the PSF and background for the image which in turn requires knowledge of the properties of \Sources on the image. An iterative procedure is, therefore, adopted to generate an estimate of the background and PSF and \Source catalogs that comprises \hyperref[sec:apPSFBackground]{}  and \hyperref[sec:apSourcemeasurement]{}

Single Frame Processing pipeline will be implemented as a flexible framework where new processing steps can be added without modifying the stack code. It should be possible for this pipeline or a subset of this pipeline to be run at the telescope facility during commissioning and operations.  

%SFM pipeline functions include:
%\begin{itemize}
%\item Assembly of per-amplifier images to an image of the entire CCD;
%\item Instrumental Signature Removal;
%\item Cosmic ray rejection and snap combining;
%\item Per-CCD determination of zeropoint and aperture corrections;
%\item Per-CCD PSF determination;
%\item Per-CCD WCS determination and astrometric registration of images;
%\item Per-CCD sky background determination;
%\item Source detection and measurement on single frame images
%\item Generation of metadata required by the OCS
%\end{itemize}

Calibrated exposure produced by the SFM pipeline must possess all information necessary for measurement of source properties by single-epoch Object Characterization algorithms.

\noindent{\bf Actions in case of failure:} In the case camera data are not available due to a network outage that is longer than the data buffer at the summit the single frame processing will work on raw images (i.e.\ without the camera pixel corrections) and be run in a batch mode. For this case crosstalk (ref) and bias subtraction will be included within the ISR stage.

\subsubsection{Input Data}
\label{sec:apSFMInput}

\paragraph*{Raw Camera Images:} Amplifier images that have been corrected for crosstalk and bias by the camera software All images from a visit should be available to the task (including snaps). An approximate WCS is assumed to be available as metadata with an absolute pointing uncertainty (for a full focal plane) of 2 arcseconds (OSS-REQ-0298) and the field rotation known to an accuracy of 32 arcseconds (LTS-206).
\begin{note}
question into Andy R about Camera operations
\end{note}


\paragraph*{Reference Catalog:} A full-sky astrometric and photometric reference catalog of stars derived either from an external survey (e.g.\ Gaia) or from the Data Release Processing. Given the current GAIA data release timeline the reference catalog is expected to have an astrometric uncertainty of $<0.5$ milliarcseconds and a photometric uncertainty of $<$20 millimag (for a $V=19$ G2V star). The expected release of these calibration catalogs in 2018 and will be derived from the GAIA spectrophotometric observations of non-variable sources.

\paragraph*{Calibration Images:} Flat-field calibration images for all passbands and all CCDs appropriate for the time at which the observations were undertaken. No corrections will be made in the flat-fields for non-uniform pixel sizes - the flat-fields will correct to a common  surface brightness. Fringe frame calibration images scaled to an expected amplitude expected for the sky conditions.
\begin{note} What assumptions will we make about SEDs for the flats. Are we assuming we know the sky SED for fringe frames \end{note}

\paragraph*{Image Metadata:} List of the positions and extents of sensor defects for all CCDs within the focal plane; electronic parameters for all CCDs (saturation limits, readnoise parameters, electronic and physical footprint for the sensors, linearity functions, brighter-fatter model, and parameterized models for component WCS (e.g.\ optical distortion model) as needed.

\subsubsection{Output Data}

\paragraph*{CalExp Images:} A calibrated exposure (CalExp) is an \hyperref[sec:spImagesExposure]{Exposure} object. The CalExp will contain a a representation of the PSF, the WCS (possibly at an optical component level), a photometric calibration object (PhotoCalib),  and a model for the  background. The pixel data will include the image, mask pixels, and a per-pixel variance estimate. For the alert production it is not expected that a model for the per-pixel covariance will be persisted but this will be revisited given the performance of the adopted image subtraction and anomaly characterization procedures (ref).
\begin{note} Covariance per pixel? \end{note}

\paragraph*{Source Catalogs:} A catalog of Sources with measured features (as described in \ref{sec:apSourcemeasurement}). 

\paragraph*{Derived Metadata} A parameterization of the PSF, WCS, photometric zeropoint, and depth for each CCD in a visit. These data will be made available to the telescope Observatory Control System (OCS) to assess the success of each observation. It is expected that these data will be persisted within a database that will be queried by the OCS.


\subsubsection{Instrumental Signature Removal}~
\label{sec:apISR}
Instrumental Signature Removal characterizes, corrects, interpolates and flags the camera (or raw) images to generate a flat-fielded and corrected exposure.

\paragraph{Pipeline Tasks}
\begin{itemize}
\item Mask the image defects at the amplifier level based on the sensor defect lists, and the per CCD saturation limits
\item Assemble the amplifiers into a single frame (masking missing amplifiers)
\item Apply full frame corrections: dark current correction, flat field to preserve surface brightness, fringe corrections
\item Apply pixel level corrections: apply a correction model for brighter-fatter to homogenize the PSF, correct for static pixel size effects based on a model
\item Interpolate across defects and saturated pixels assuming a model for the PSF (with a nominal FWHM)
\item Apply a cosmic ray detection algorithm as described in \hyperref[sec:artifact]{sec:artifact}
\item Generate a summed and difference image from the individual snaps propagating the union of the mask pixels in each snap
\end{itemize}

\begin{note} What will we know about the PSF prior to measuring it - will we need to rerun interpolation after deriving the PSF or can we use a previously defined PSF. Do we interpolate CRs \end{note}

Dependent on the properties of the delivered LSST image quality for 15 second snaps it may be advantageous to model any bulk motion between snaps prior to combination.


\subsubsection{PSF and background determination}~
\label{sec:apPSFBackground}

Given exposures that have been processed through Instrument Signature Removal, \Sources must be detected to determine the astrometric and photometric calibration of the images. As noted previously an iterative procedure will be adopted to generate an estimate of the background and PSF, and to characterize the properties of the detected sources.  Convergence criteria for this procedure are not currently defined. The default implementation assumes three iterations.

\paragraph{Pipeline Tasks}
The iterative process for PSF and background estimation comprises
\begin{itemize}
\item Background estimation on the scale of a single CCD is as described in \hyperref{sec:acBackgroundEstimation}, which divides the CCD into subregions and estimates  a robust mean for non-source pixels.
\item Subtraction of the background and the detection of sources as described in \hyperref{sec:acSourceDetection}. The initial detection threshold will be 5$\sigma$ with  $\sigma$ the estimated from variance image plane.
\item Measurement of the properties of the detected sources (see \hyperref[sec:apSourcemeasurement]{sec:apSourcemeasurement})
%\item Single CCD \hyperref[sec:acDeblending]{Deblend sources}
\item Selection of isolated PSF candidate stars based on a signal-to-noise threshold (default 50 $\sigma$) \item Single CCD PSF determination using \hyperref[sec:acSingleCCDPSF]{} and given the selected bright sources 
\item Masking of source pixels within the CCD (growing the footprint of the \Sources to mask the outer regions of the \Source profiles will likely be required)
%\item Single CCD \hyperref[sec:acModelSpatialPSF]{PSF spatial model}
\end{itemize}

The default expectation is that all tasks within this procedure would be iterated until convergence.  There maybe significant optimizations to be gained by excluding the \Source detection step after an initial detection if the number of sources does not change significantly.

\begin{note} Treatment of covariance \end{note}


\subsubsection{Source measurement}~
\label{sec:apSourcemeasurement}

For the \Source catalog generated in \ref{sec:apPSFBackground} source properties are measured using a subset of features described in \ref{sec:acMeasurement}. Source measurement is for all sources within the Source catalog and not just the bright subset used to calibrate the PSF.  We anticipate using the following plugin algorithms,
\begin{itemize}
\item Centroids based on a static PSF model fit (see \hyperref[sec:acCentroidAlgorithms]{} and \hyperref[sec:acStaticPointSourceModels])
\item Aggregation of pixel flags as described in \hyperref[sec:acPixelFlags]{}
\item Aperture Photometry as geven in \hyperref[sec:acAperturePhotometry]{} (but only for one or two radii) 
\item PSF photometry given in \hyperref[sec:acPSFPhotometry]{} assuming a static PSF model fit
\item  An aperture correction estimated assuming a static PSF model and measurement of the curve of growth for  detected sources as given in \hyperref[sec:apertureCorrection]{}
\end{itemize}

\begin{note} why only one or two radii \end{note}


\subsubsection{Photometric and Astrometric calibration}~ 
Photometric and astrometric calibration entails a ``semi-blind'' cross match of a reference catalog derived either from the DRP \Objects or from an external catalog (see \hyperref[sec:apSFMInput]{}), the generation of a WCS (on the scale of a CCD or visit), and the generation of a photometric zeropoint (on the scale of a CCD).

\paragraph{Pipeline Tasks}
The photometric and astrometric calibration is expected to be performed at the scale of a single sensor. It is possible that the calibration process will need to be extended to larger scales (up to a single visit) if there is significant structure in the photometric zero point, or if astrometric distortions cannot be calibrated at the scale of the CCD with sufficient accuracy such that that the astrometric distortions do not dominate the false positives in the image subtraction.

A visit level calibration strategy will introduce synchronization points within the processing of the CCDs as the detections on all CCDs will need to be aggregated prior to the astrometric fit.

The procedures used to match and calibrate the data are,
\begin{itemize}
\item CCD level source association between the DRP reference catalog
  (or external catalog) and \Sources  during the PSF and background
  estimation using a simplified TaberB approach described in \hyperref[sec:acSingleCCDReferenceMatching]{}. Given an astrometric accuracy of $<0.5$ milliarcseconds from external catalogs such as GAIA (for a $V=19$ G2V star) or a $<50$ milliarcseconds for the DRP catalogs the search radii for sources will be small {\bf NUMBER}
\item Generation of a photometric fit on the scale of a single CCD as described in \hyperref[sec:acSingleCCDPhotometricFit]{}
\item Decomposition of the astrometric components (e.g.\ optical distortions, sensor tree-rings) for a single CCD and generation of  an astrometric fit at the level of a single CCD (or visit) using the tasks given in \hyperref[sec:acSingleCCDAstrometricFit]{}
\item Persistence of the astrometric, PSF, and photometric solutions that will be made available to the OCS 
\end{itemize}

Given the number of stars available on a CCD or the complexity of the astrometric solutions for the LSST it may be necessary that the astrometric and photometric solutions must be performed at the level of a visit and not a CCD.  For these cases the operations will be \hyperref[sec:acSingleVisitReferenceMatching]{single visit matching},   \hyperref[sec:acSingleCCDPhotometricFit]{single visit photometric solutions}, \hyperref[sec:acSingleVisitAstrometricFit]{single visit astrometric fits}.

Astrometric and photometric performance within crowded fields will require that the order of the WCS should depend on the number of calibration Sources that are available.

\begin{note} What about high density regions? \end{note}

\subsection{Alert Detection (\wbsDiffim)}
\label{sec:apAlertDetection}

Alert Detection identifies variable, moving, and transient sources within a calibrated exposure by subtracting a deeper template image. Sources, DIASources, detected on a DIAImage are associated with known Objects (including SSObjects that have been propagated to the date of the CalExp exposure) and their properties measured. The resulting DIAImages and DIAObjects will be persisted by the Alert Detection pipeline.

Alert Detection pipeline shall difference, and detect and characterize sources in the time required to achieve the 60 second design goal for nightly processing (current timing allocation: 24 seconds). The algorithms employed by the pipeline shall result in purity and completeness of the sample as required by the \DMSR\@. Image differencing shall perform as well in crowded as in uncrowded fields. 



The process for image differencing requires the following steps
\begin{itemize}
\item Creation or \hyperref[sec:acRetrieveTemplate]{retrieval} of a TemplateCoadd.
\item Matching the astrometry and PSF of a CalExp to the  and subtracting the template image from the CalExp. This will require accounting for the relative differences in image quality and noise between the template and CalExp.
\item (optional) Dependent on the density of sources \hyperref[sec:acSingleFrameDeblending]{deblending} of difference images may be necessary
\item Measurement of image difference sources (accounting for whether the CalExp image was preconvolved or not)
\item Removal of spurious DIASources 
\item Association of the DIASources with previously detected DIAObjects and SSObjects
\item Forced measurement for all DIAObjects on the DIAImage
\end{itemize}

For all DIAObjects forced photometry will be undertaken given a time windowed estimate of the DIAObject position.

\subsubsection{Input Data}
\label{sec:apADInput}

\paragraph*{CalExp Images:} Calibrated exposure processed through \ref{sec:apSingleFrameProcessing} with associated WCS, PSF, mask, variance, and background estimation.

\paragraph*{Coadd Images:} TemplateCoadd images that spatially overlap with the CalExp images processed through \ref{sec:apSingleFrameProcessing}. This coadded image is optimized for image subtraction and is expected to be characterized in terms of a tract/patch/filter. Generation of this template may account for differential chromatic refraction or be generated for a limited range of airmass, seeing, and parallactic angles.

\paragraph*{Object Catalogs:} \Objects that spatially overlap with the CalExp images processed through \ref{sec:apSingleFrameProcessing}. This \Object catalog will provide the source list for determining nearest neighbors to the detected DIASources. 

\begin{note} We should selected nearest 3 stars and nearest 3 galaxies \end{note}

\paragraph*{DIAObject Catalogs:} \DIAObjects that spatially overlap with the CalExp images processed through \ref{sec:apSingleFrameProcessing}. This DIAObject catalog will provide the association  list against which the DIASources will be matched. 

\paragraph*{SSObject Catalogs:} The \SSObject list at the time of the observation. The SSObject positions will be propagated to the date of the CalExp observations and will provide an association  list for cross-matching against the detected \DIASources to identify known Solar System objects.

\paragraph*{Reference classification catalogs:} Classification of \DIASources based on their morphological features (and possibly estimates of the local density or  environment associated with the DIASource) will be undertaken prior to association in order to reduce the number of false positives. The data structures that define these classifications will be required as an input to the spuriousness analysis. 



\subsubsection{Output Data}
\label{sec:apADOutput}

\paragraph*{DIAImage Images:} Image differences (DIAImage) derived from subtracting a CalExp from a TemplateCoadd image 

\paragraph*{DIASource Catalogs:} Sources detected and measured from the DIAImages (DIASources) using the set of parameters described in table \hyperref[table:ap_features] will be persisted


\paragraph*{DIAObject Catalogs:} DIASource will be associated with existing DIAObjects and persisted. New DIASource (i.e.\ those not persisted) will generate a new instance of a DIAObject 

\paragraph*{DIAForcedPhotometry Catalogs:}




\subsubsection{Template Generation}~
\label{sec:apCRTemplates}

Template generation requires the creation or \hyperref[sec:acRetrieveTemplate]{retrieval} of a TemplateCoadd w that is matched to the position and spatial extent of the input CalExp. Generation of the TemplateCoadd could be from a persisted Coadd that was generated from CalExp exposures with comparable (within a predefined tolerance) airmass and parallactic angles or from a model that corrects for the effect of  \hyperref[sec:acDCRTemplates]{differential chromatic refraction}. It is expected that these operations would be undertaken on a CCD level but for efficiency the TemplateCoadd might be returned for a full visit. 


\paragraph{Pipeline Tasks}
\begin{itemize}
\item Query for a TemplateCoadd images that are within a given time interval of the CalExp  (default 2 years) of the current sensor image, and are within a specified airmass and parallactic angle
\item (optional) Derive an airmass and DCR corrected TemplateCoadd from a model (see  \hyperref[sec:acDCRTemplates]{ DCR template generation}). The direction of the DCR correction will be aligned with the   ``parallactic angle'' of the CalExp image
\end{itemize}

\subsubsection{Image differencing}~

Image differencing incorporates the matching of a CalExp to a TemplateCoadd (astrometrically and in terms of image quality), subtraction of the tempalte image, detection and measurement of DIASources, removal of spurious DIAsources, and association of the DIASources with previously identified DIAObjects, Objects, and SSObjects. 

\paragraph{Pipeline Tasks}
\begin{itemize}
\item Determine a relative astrometric solution from the WCS of the TemplateCoadd image and CalExp image
\item Match \hyperref[sec:apSourcemeasurement]{DRP} sources for the TemplateCoadd against sources from \hyperref[sec:apSingleFrameProcessing]{SFP} of the raw images 
\item \hyperref[sec:acWarping]{Warp or resample} the TemplateCoadd to match the astrometry of the CalExp. It is possible that the an operation to astrometrically match the TemplateCoadd and CalExp using faint source will need to be undertaken dependent on the accuracy of the WCS.
\item For CalExp images with an image quality that is better than the TemplateCoadd preconvolve the CalExp image with the PSF use a  \hyperref[sec:spKernels]{convolution kernel}. This reduces the significance of any deconvolution in the PSF matching.
\item \hyperref[sec:acDiffImDecorrelation]{Match} the PSF of the CalExp and TemplateCoadd images and construct a spatial model for the matching kernel. This approach may include matching to a common PSF through \hyperref[sec:acPSFHomogenization]{homogenization} 
\item Apply the matching kernel and \hyperref[sec:acImageSubtraction]{subtract} the images to generate a DIAImage
\item \hyperref[sec:acDiffImDecorrelation]{Decorrelate} the DIAImage to reduced the correlations in the noise due to the convolution with the image differencing matching kernel. 
\item \hyperref[sec:acSourceDetection]{Detect DIAsources} on the DIAImage. Convolution with a detection kernel will depend on whether the CalExp was preconvolved in step 3
\item \hyperref[sec:acDiffImMeasurement]{ Measure sources} on the DIAImage including \hyperref[sec:acDipoleModels]{dipole models}. The specific algorithms used for measurement of DIASources will depend on whether the CalExp image was preconvolved.  Source measurements will include: \hyperref[sec:acDipoleModels]{dipole fit}, \hyperref[sec:acTrailedPointSourceModels]{trailed source} measurement
\item Measure flux on snap difference for all DIASources
\item \hyperref[sec:acSpuriousnessAlgorithms]{Spuriousness algorithms} also known as ``real-bogus'' may have to be applied at this time dependent on the number of false positives. DIASources classified as spurious at this stage may not be persisted (dependent on the density of the false positives). The default technique will be based on a trained random forest classifier. It is likely that the training of this classifier will need to be conditioned on the image quality and airmass of the observations.
\end{itemize}

\subsubsection{Source Association}~

In Source Association DIASources detected within a given CCD will be cross-matched (\hyperref[sec:acDIAObjectGeneration]{associated}) with the DIAObject table and the SSObjects (whose ephemerides have been generated for the time of the current observation. The association will be probabilistic  and account for the uncertainties within the positions. Dependent on the available computational resources the association may include flux and priors on expected proper motions for the sources. External targets (e.g.\ well resolved transient events from other telescopes or instruments) can be incorporated within this component of the nightly pipeline enabling either matching to DIASources or generation of forced photometry at the position of the external source. It is not clear that this is an objective of the pipeline. 


\paragraph{Pipeline Tasks}
\begin{itemize}
\item The \hyperref[sec:acEphemerisCalculation]{ephemerides} for SSObjects will be generated for those sources overlapping a DIAImage
\item \hyperref[sec:acDIAObjectGeneration]{Source association} will be undertaken for all DIASources. Matching will be to DIAObjects, and the ephemerides of SSObjects. Positions for DIAObjects will be based on a a time windowed average of the DIASources that make up the DIAObject. A probabilistic association will need to account for one-to-many and many-to-one associations. It may be necessary to generate joint associations across all DIAObjects (and associated DIASources) in the local  vicinity of a DIASource to correct for mis-assignment from previous observations. This could include the pruning and reassignment of DIASources between DIAObjects.
\item DIASources will be \hyperref[sec:acDIAObjectGeneration]{associated} with the Object table from DRP. In its simplest case this will be a nearest neighbor search that will define a set of nearest neighbors (the default radius for association is not defined) that will be persisted with DIAObjects as a measure of local environment. More sophisticated association may be undertaken to match the DIASources to Objects in order to enable access to the DRP properties of a source (e.g.\ proper motion and parallax - see below).
\item DIASources unassociated with a DIAObject will instantiate a new DIAObject
\item The agregate positions for the DIAObjects will be updated based on a rolling time window (default 30 days). 
\item (optional) The proper motion and parallax will be \hyperref[sec:acStellarMotionFitting]{updated}. It is not currently clear if there is a science case for generating proper motions and parallaxes within the DIAObjects if the DRP Objects are available for each source. 
\end{itemize}


\subsubsection{Prototype Implementation}

The prototype code is available at \url{https://github.com/lsst/ip_diffim}. The current prototype, while functional, will require a partial redesign to be transfered to construction to address performance and extensibility concerns.

\clearpage

\subsection{Alert Distribution Pipeline (\wbsAP)}

The Alert Distribution Pipelines takes the newly discovered \DIAObjects (including their associated historical observations) and all related metadata as described in the \DPDD, and deliver alert packets in \VOEvent format to a variety of endpoints via standard IVOA protocols (eg., \VOEvent Transport Protocol; VTP\@). Packaging of the event will include the generation of postage stamp cutouts (30x30 pixels on average) for the difference image and the template image. For an event rate of about 1.5x10$^7$ (including false positives)l we expect the compressed \VOEvents to amount to approximately 600GB of data per night per \VOEvent stream (assuming no filtering of the data). 

To directly serve the end-users, the Alert Generation Pipeline will provide a basic, limited capacity, alert filtering service. This service will run at the LSST U.S. Archive Center (at NCSA). It will enable astronomers to create simple filters that limit what alerts, and what fields from those alerts, are ultimately forwarded to them. These \emph{user defined filters} will be configurable with a simplified SQL-like declarative language, or short snippets of (likely Python) code.

\VOEvent alerts will be persisted in an alert database as well as distributed through a message queue. The alert database (AlertDB) will be synchronized at least once every 24 hours and will be queriable by external users. The message queue that distributes the alerts is expected to have the capability  to replay events given a break in the network connection but not to support general queries.

\subsubsection{Input Data}
\label{sec:apADInput}

\paragraph*{DIAObject catalog:} \DIAObjects generated through image differencing and association with existing \DIAObjects will be used to create alert packets 

\paragraph{Difference Images:} The DIAImage will be used to  generate postage stamp or cut out images of DIAObjects within the CCD 

\paragraph{Coadd Images:} The TemplateCoadd used in the image subtraction will be used to  generate postage stamp or cut out images of DIAObjects   within the CCD


\subsubsection{Output Data}

\paragraph{\VOEvent Catalog:} \VOEvents generated from the DIAObjects and cutouts will be persisted. The form of these persisted events has not been decided.



\subsubsection{Alert postage stamp generation}~

Creates the associated image cutouts (30x30 pixels on average) for all detect DIAObjects (cutouts are generated from the current observation and not from historical observations).

\paragraph{Pipeline Tasks}
 \begin{itemize}
\item Extract from the DIAImage the a cutout of each DIAObject with a DIASource detected within the current observation.  Cutout images will be scaled to the size of the DIASource but on average will be 30x30 pixels. Variance and mask planes, WCS, background model, and associated metadata will be extracted. The prototype implementation assumes that these cutouts will be persisted as FITS images. The projection for these images will be the native projection of the DIAImages.

\item Extract from the TemplateCoadd  a cutout of each DIAObject with a DIASource detected within the current observation.  Cutout images will be identical in size and footprint as those derived from the DIAImage. Variance and mask planes, WCS, and associated metadata will be extracted with the pixel data. The prototype implementation assumes that these cutouts will be persisted as FITS images. The projection for these images will be the native projection of the DIAImages.
\end{itemize}

\subsubsection{Alert queuing and persistance}
\label{sec:apQueue}
Distributes and persists the DIAObject events as \VOEvents through a message queue together with a {\it limited} filtering interface. Persists the \VOEvents in the AlertDB. Event message stream and the AlertDB are synchronized at least once every 24 hours.


\paragraph{Pipeline Tasks}
\begin{itemize}
\item Publish DIAObjects to a caching message queue (e.g.\ \hyperref[http://kafka.apache.org]{Apache Kafka}) through the butler. The prototype implementation assumes a distributed and partitioned messaging system that uses a \hyperref[https://en.wikipedia.org/wiki/Publish_subscribe_pattern]{publication-subscription} model for communication of DIAObjects. This model maintains feeds of messages in categories called topics. An example topic would be a DIAObject. Whether a topic would comprise a full DIAObject or a subset of the data remains open. A topic model with subsets of parameters would require that consumers of the topics would be able to synchronize and join topics into a full DIAObject. For each of the 189 CCDs, approximately 75, assuming a 50\% false positive rate, events will be passed as messages to the messaging queuing system in an asynchronous manner.
\item A consumer layer will subscribe to the  message queue and package them as \VOEvents and distribute these events to external users. To account for network outages between the message queue and the consumer the message queue must be able to replay previous events. 
\item  The consumer layer will provide a command line API to define simple queries or filtering of the events that will be distributed (limited to querying on existing DIAObject fields, or filtering the attributes of the DIAObject that will be packaged within a \VOEvent. Web-based interfaces to the consumer layer will be developed by SUIT. 
\item Filtered DIAObjects are packaged into \VOEvents and broadcast to VOEvent clients through the consumer layer
\item A full, unfiltered, VOEvent alert stream will be broadcast to the AlertDB using the consumer layer. 
\item Prior to the start of the following nights observations the message queue will be flushed and synchronized with the AlertDB. It is possible to persist the message queue on longer timescale but will require synchronization with 24 hours of the observations to meet the requirements specified in the DPPD.
\end{itemize}


To cope with the variation in density of events as a function of position on the sky and the need for fault tolerance the message queue will need to be able to partition and replicate data. Each full DIAObject stream will require about 1 Gb/s network capacity. Whether the consumer layer will instantiate a new consumer for each filter (or client) or will orchestrate the \VOEvents from a single subscription to the message queue is an open question that will depend on the expected network topology (internal and external to the data center at NCSA).

The AlertDB will have an interface that can be queried (to enable historical searches of events) including searches on other than timestamps. It is expected that the AlertDB will be a noSQL datastore (e.g.\ Cassandra).

%partitions must be ordered the same, failover must let a consumer to move to a different partion and have the messages ordered


\subsubsection{Forced Photometry on all DIAObjects}~

Generate forced (PSF) photometry on the DIAImage for all DIAObjects that overlap with the footprint of the CCD. Forced photometry is only generated for DIAObjects for which there has been a DISource detection within the last 12 months. The forced photometry is persisted in the forced photometry table in the level 1 database. Alerts are released prior to the generation of forced photometry and forced photometry is not released as apart of an alert which means that this component of the processing is not subject to the 60 second processing requirements for nightly processing.

\paragraph{Pipeline Tasks}
\begin{itemize}
\item Extract all DIAObjects within the level 1 database for which these is no associated DIAObject. This information is available from the DIASource and DIAObject association.  DIAObjects within the last 12 months for which there have been no DIASource detections will be excluded from the forced photometry calculation
\item For the aggregate positions within the DIAObject undertake a PSF forced measurement as described in section \hyperref[sec:acForcedMeasurement]{}
\item Update the forced photometry tables in the level 1 database.
\end{itemize}



\clearpage

\subsection{Precovery Photometry Pipeline}

Within 24 hours, forced photometry will be performed on all unassociated DIASources within an image (i.e.\ new DIAObjects). For each DIAObject forced (PSF) photometry will be measured at the position of the source in the preceding DIAImages (for a 30-day window prior to the observation)

\subsubsection{Input Data}

\paragraph*{DIAImages:} A cache of DIAImages within finite time interval (default 30 days)  of the previous nights observations

\paragraph*{DIAObject Catalog:} All unassociated (new) DIAObjects observed within the previous night

\begin{note} We associate (measure forward) for 12 months but look back for 30 days\end{note}

\subsubsection{Output Data}

\paragraph*{DIAObject Forced Photometry:} Updated forced photometry table for all new DIAObjects


\paragraph{Pipeline Tasks}
\begin{itemize}
\item Extract from the Level 1 database all DIAObjects that were unassociated (i.e.\ new DIASource detections) from the previous nights reduction
\item Extract DIAImages within a  default 30 day window prior to the observation
\item Force photometer the extracted images as described in \ref{sec:acForcedMeasurement} using a PSF model and the centroid defined in the DIAObject
\item Update the forced photometry table within the Level 1 database
\end{itemize}
\clearpage



\subsection{Moving Object Pipeline (\wbsMOPS)}

The Moving Object Pipeline (MOPS) is responsible for generating and managing the Solar System\footnote{Also sometimes referred to as `Moving Object'} data products. These are Solar System objects with associated Keplerian orbits, errors, and detected \DIASources. Quantitatively, it shall be capable of detecting 95\% of all Solar System objects that meet the findability criteria as defined in the \OSS\@. 


Components of MOPS are run during and separately from nightly processing. MOPS for nightly processing is described in \ref{xxx} as part of the source association. ``Day MOPS'' processes the newly detected DIASources (or DIAObjects) to search for candidate asteroid tracks. The procedure for ``Day MOPS'' is to link DIASource detections within a night (called tracklets), to link these tracklets across multiple nights (into tracks), to fit the tracks with an orbital model to identify those tracks that are consistent with an asteroid orbit, to match these new orbits with existing SSObjects, and to update the SSObject table. By its nature this process is iterative with DIASources being associated and disassociated with SSObjects. The frequency of the iterations is an open area.

\begin{note} Specify the expected number of sources \end{note}

\subsubsection{Input Data}

\paragraph*{DIASource catalog: } Unassociated DIASources from the previous night of observing.  This means DIASources that have not been associated with a DIAObject.  DIASources associated with an SSObject in the night are still passed through the MOPS machinery

\paragraph*{SSObject catalog: } The catalog of known solar system sources

\paragraph*{Exposure metadata:} A description of the footprint of the observations including the positions of bright stars or a model for the detection threshold as a function of position on the sky (including gaps between chips)


\subsubsection{Output Data}

\paragraph*{SSObject catalog: } An updated SSObject database with SSObjects both added and pruned as the orbital fits are refined

\paragraph*{DIASource database:} A updated DIASource database with DIASources assigned and unassigned to SSObjects


\subsubsection{Tracklet identification}~
From multiple visits within a night, link unassociated DIASources to form pairs (or more) of DIASources

\paragraph{Pipeline Tasks}
\begin{itemize}
\item Extract unassociated DIASources from the Level 1 database
\item Link DIASources into tracklets assuming a maximum velocity for the moving sources. The maximum velocity will be based on a prior as described in  \cite{kubica05}. For each tracklet a velocity vector will be calculated to enable pruning or merging of degenerate tracklets within a data set. Tracklets can contain multiple points. In the process of merging tracklets DIASources that are not a good fit for the merged tracklet will be remove and their associated tracklets returned to the tracklet database.  Moving or trailed sources will incorporate the position angle of the source when linking. Details of the implementation of the DIASource linkage is described in \ref{sec:acMakeTracklets}
\item Temporarily persist a database of tracklets. This database will be required for at least 30 days of data but, depending on resources available, may persist for longer.
\end{itemize}


\subsubsection{Precovery and merging of tracklets}~
Tracklets are matched and merged with existing \SSObjects and removed from the Tracklet database. This culls any tracklets or DIASources that obviously belong to an existing SSObject from the rest of the processing. 

\paragraph{Pipeline Tasks}
\begin{itemize}
\item Return all tracklets identified within a given night of observations
\item Return the footprints of each visit and the time of the observation
\item Extract SSObjects from the SSObject database and propagated those orbits to the position and time of a visit. Details of this orbit propagation for precovery are described in \ref{sec:acEphemerisCalculation}. The computational requirements for this precovery are substantial and efficient mechanisms for indexing the SSObject orbits will be necessary. Trajectories of known asteroids could be pre-calculated and modelled as polynomials to enable fast interpolation during day MOPS.
\begin{note}What are the numbers for the SSObject propagation and number of tracklets per visit\end{note}
\item Merge (precovery) the tracklets with the projected SSObject trajectories and refit  the SSObject orbit model. DIASources previously associated with an SSObject may no longer fit the updated SSObject orbits. These DIASources will be removed from the SSObject and returned as unassociated DIAObjects to the level 1 database. All tracklets associated with these DIAObjects will be  returned to the tracklet database. Details of this attribution and precovery are described in \ref{sec:acAttributionAndPrecovery}
\end{itemize}

\subsubsection{Linking tracklets and orbit fitting}~

Given a database of tracklets constructed from a window (default 30 days) of time, link the tracklets into tracks assuming a quadratic approximation to the trajectory. Fit these tracks with orbital models and update the SSObject database.

\paragraph{Pipeline Tasks}
\begin{itemize}
\item Extract all tracklets from the tracklet database for a specified window in time (default 30 days)
\item Merge tracklets into tracks based on their velocities and accelerations. Candidate tracks are pruned by fitting a quadratic relation to the positions (after applying a topocentric correction to the positions of the sources). Efficiency in this matching procedure is provided by a spatial index such as a kd-tree (see \ref{sec:acOrbitFitting}). 
\item Fit an orbit to each candidate track using a tool such as OOrb (https://github.com/oorb/oorb) and, for poorly fitting  points, return the DIASources and associated tracklets to their respective databases for subsequent reprocessing.
\item Merge SSObjects that have similar orbital parameters based on range searches within the six dimensional orbital parameter space.  Merged SSObjects will need to be refit and any poorly fitting DIASources (and associated tracklets) returned  to their respective databases for subsequent reprocessing. Details of this procedure are given in \ref{sec:acOrbitMerging}
\end{itemize}

\subsubsection{Global precovery}~
For all new or updated SSObjects propagate the orbits to the positions and times of the observations of all tracklets and orphan DIAObjects to ``precover'' further support for the orbits. This will prune the number of tracklets and DIAObjects that will require merging in subsequent observations. 

\paragraph{Pipeline Tasks}
\begin{itemize}
\item Return all tracklets identified within a given night of observations
\item Return the footprints of each visit and the time of the observation
\item Extract orbits for all new or updated SSObjects and propagate the positions to the times of the observations for all visits covering the extent of the tracklet database, default 30 days, (see \ref{sec:acEphemerisCalculation})
\item Merge the tracklets with the projected SSObject positions and refit the SSObject orbit model. Poorly fitting DIASources (and associated tracklets) will be removed from the SSObject and returned as unassociated DIAObjects to the Level 1 database (as described in\ref{sec:acAttributionAndPrecovery}).
\end{itemize}


The process for precovery and updating of the SSObject models is naturally iterative (given the pruning of poorly fitting DIAObjects and tracklets). Updates of the SSObjects as part of each night of operations should enable sufficient iterations without requiring Day-MOPS to be rerun multiple times per day. The computationally expensive operations in this pipeline are the orbit propagation and the orbit fitting. Resources required for orbit propagation could be reduced be removing the initial precovery stage but at the cost of increasing the number of tracklets that would be available for matching into tracks. Extending the Global precovery to include singleton DIASources (i.e.\ one that are not merged into tracklets) would enable the identification of asteroids at the edge of the nightly footprint (where an object moves outside of the nightly survey footprint prior to the second visit or a second visit is not obtained for a given field).

\subsubsection{Prototype Implementation}

Prototype MOPS codes are available at
\url{https://github.com/lsst/mops_daymops} and
\url{https://github.com/lsst/mops_nightmops}. We expect it will be
possible to transfer a significant fraction of the existing code into
Construction. Current DayMOPS prototype already performs within the
computational envelope envisioned for LSST Operations, though it does
not yet reach the required completeness requirement.


