\section{Level 1 Pipelines}
\label{sec:ap}

\begin{figure}[th]
\begin{center}
\includegraphics[width=0.9\textwidth]{figures/Level_1_Processing_Flowchart.jpg}
\caption{\label{fig:nightly} The nightly processing flowchart
  describing the flow of images and data through single frame
  processing, image differencing, alert generation and production}
\end{center}
\end{figure}

\subsection{Single Frame Processing Pipeline (\wbsSFM)}
\label{sec:apSingleFrameProcessing}

Single Frame Processing (SFM) Pipeline is responsible for reducing raw
or camera corrected image data to \emph{calibrated exposures}, and the
detection and measurement of \Sources (using the components
functionally a part of the Object Characterization Pipeline).

\subsubsection{Key Requirements}

SFM pipeline functions include:
\begin{itemize}
\item Assembly of per-amplifier images to an image of the entire CCD;
\item Instrumental Signature Removal;
\item Cosmic ray rejection and snap combining;
\item Per-CCD determination of zeropoint and aperture corrections;
\item Per-CCD PSF determination;
\item Per-CCD WCS determination and astrometric registration of images;
\item Per-CCD sky background determination;
\item Source detection and measurement on single frame images
\item Generation of metadata required by the OCS
\end{itemize}

Calibrated exposure produced by the SFM pipeline must possess all
information necessary for measurement of source properties by
single-epoch Object Characterization algorithms.

It shall be possible to run this pipeline in two modes: a ``fast" mode
needed in nightly operations for Level 1 data reductions where no
source characterization is done beyond what's required for zero-point,
PSF, sky, and WCS determination (image reduction); and a ``full" mode
that will be run for Level 2 data reductions.  It should be possible
for this pipeline or a subset of this pipeline to be run at the
telescope facility during commissioning and operations.  \emph{AJC: is the level 2 a requirement}


\subsubsection{Baseline Design}

Single Frame Processing pipeline will be implemented as a flexible
framework where different data can be easily treated differently, and
new processing steps can be added without modifying the stack code.

It will consist of three primary components:
%
\begin{itemize}
\item A library of useful methods that wrap a small number of atomic operations (e.g., {\tt interpolateFromMask}, {\tt overscanCorrection}, {\tt biasCorrection}, etc.) % RHL things like overscanCorrection aren't atomic (or at least, they use afw::math and afw::cameraGeom primitives)
\item A set of classes ({\tt Task}s) that perform higher level jobs
    (e.g., {\tt AssembleCcdTask}, or {\tt FringeTask}), and a top level class to apply corrections to the input data in the proper order. This top level class can be overridden in the instrument specific {\tt obs\_*} packages, making the core SFM pipeline camera agnostic.
\item A top-level Task to run the SFM pipeline.
\end{itemize}

In the paragraphs to follow, we describe the adopted baseline for key SFM algorithms. If not discussed explicitly, the algorithmic baseline for all other functionallity is assumed to be the same as that used by SDSS \emph{Photo} pipeline \cite{LuptonPhoto}.

\paragraph{Instrumental Signature Removal:}~

\noindent
{\bf Input Data:}
\begin{itemize}
\item Camera corrected images (crosstalk, overscan, linearity)
\item Sensor defect lists
\item Metadata including electronic parameters (saturation limits,
  readnoise, electronic footprint)
\end{itemize}

\noindent
{\bf Output Data:}
\begin{itemize}
\item Calexp images
\end{itemize}

\noindent 
{\bf Actions in case of failure:}\\
In the case camera data are not available due to a network outage that
is longer than the data buffer at the summit the ISR processing will
work on raw images (i.e.\ without the camera corrections) and be run
in a batch mode.

\noindent 
{\bf Alternative procedures:}\\

\noindent
{\bf Subtasks:}
\begin{itemize}
\item Mask defects and saturation
\item Assembly
\item Full frame corrections: Dark, Flats (includes fringing)
\item Pixel level corrections: Brighter fatter, static pixel size effects
\item {\bf QUESTION is this run prior to pixel level corrections} Interpolation of defects and saturation
\item \hyperref[sec:artifact]{CR rejection}
\item Generate snap difference
\item Snap combination
\end{itemize}


\paragraph{PSF determination and background determination:}~

Iterative procedure to fit and remove background, identify sources to
5$\sigma$, select bright sources (50$\sigma$), and fit a PSF
model. Convergence criteria for the procedure is not defined but the
default procedure assumes three iterations.

\noindent
{\bf Input Data:}
\begin{itemize}
\item Access to ICexp images from ISR 
\end{itemize}
{\bf Output Data}
\begin{itemize}
\item IC images 
\item IC source catalog 
\end{itemize}
 {\bf Actions in case of failure}\\
{\bf Alternative procedures}\\

\noindent
{\bf Subtasks:}


\begin{itemize}
\item \hyperref[sec:acBackgroundEstimation]{Background estimation}
\item \hyperref[sec:acSourceDetection]{Source detection}
%\item Single CCD \hyperref[sec:acDeblending]{Deblend sources}
\item Selection of PSF candidate stars based on signal-to-noise
  (default 50 $\sigma$) and isolate sources
\item Single CCD \hyperref[sec:acSingleCCDPSF]{PSF determination}
%\item Single CCD \hyperref[sec:acModelSpatialPSF]{PSF spatial model}
\end{itemize}

\paragraph{Source measurement:}~
\label{sec:apSourcemeasurement}
\noindent
{\bf Input Data:}\\
\begin{itemize}
\item Access to background subtracted ICexp images
\item Access to sources detected from ICexp images
\end{itemize}
{\bf Output Data:}\\
\begin{itemize}
\item IC source catalog with measurements
\end{itemize}
 {\bf Actions in case of failure}\\
{\bf Alternative procedures}\\

\noindent
{\bf Subtasks:}

Measurement on sources detected for the PSF estimation using a subset
of parameters as required by the Alert Detection. Measurements are for
all sources not just bright subset.
\begin{itemize}
\item  Source measurement - \hyperref[sec:measurement]{Single visit
    measurement} for detected sources
\item  \hyperref[sec:apertureCorrection]{Aperture correction} for
  detected sources
\end{itemize}

\paragraph{Photometric and Astrometric calibration:}~

\noindent
{\bf Input Data:}\\
\begin{itemize}
\item Access to background subtracted ICexp images
\item Access to sources detected from ICexp images
\item Butler access to DRP's internal reference catalog 
\end{itemize}
\begin{itemize}
\item  Calexp images (persisted)
\item Calibrated source catalog
\item OCS PSF, WCS, metadata (TBD) 
\end{itemize}
 {\bf Actions in case of failure}\\

\noindent
{\bf Subtasks:}

Photometric and astrometric calibration performed at the scale of a
single sensor (extended to the scale of a visit depending on required fidelity)
\begin{itemize}
\item Sensor level \hyperref[sec:acSingleCCDReferenceMatching]{source
    association} between the DRP reference catalog and sources
  detected during PSF characterization
\item CCD level \hyperref[sec:acSingleCCDPhotometricFit]{photometric solution}
\item Remove known astrometric distortions 
\item CCD level composed \hyperref[sec:acSingleCCDAstrometricFit]{astrometric
    solution} to residuals
\item Output information for OCS telemetry: WCS ACTION clarify OCS interactions
\end{itemize}

{\bf Alternative procedures}\\
Dependent on accuracy of single sensor photometric and astrometric calibrations
\begin{itemize}
\item Visit level \hyperref[sec:acSingleVisitPhotometricFit]{photometric solution}
\item Visit level composed 
  \hyperref[sec:acSingleVisitAstrometricFit]{astrometric solution} to 
  residuals if required 
\end{itemize}
\noindent


\subsubsection{Prototype Implementation}

The prototype codes are available in the following repositories: \url{https://github.com/lsst/ip_isr}, \url{https://github.com/lsst/meas_algorithms}, \url{https://github.com/lsst/meas_astrom}, \url{https://github.com/lsst-dm/legacy-meas_mosaic}, \url{https://github.com/lsst/pipe_tasks}.

\clearpage

\subsection{Alert Detection (\wbsDiffim)}
\label{sec:apAlertDetection}

\noindent 
{\bf Input Data:}\\
\begin{itemize}
\item Butler access to Coadd images from DRP that overlap spatially
  with CalExp images 
\item Access to DIAObjects that overlap spatially with CalExp images 
\item Access to Objects that overlap spatially  with CalExp images 
\item Access to SSObjects whose ephemerides overlap spatially  with CalExp images 
\item Internal reference catalog for CalExp from DRP 
\item PSF \hyperref[sec:apSingleFrameProcessing]{measured} from the
  science image.
\item Data structures for the real-bogus classifier
\end{itemize}


{\bf Output Data}\\
\begin{itemize}
\item DIAimage persisted
\item DIASources persisted
\item DIAObjects persisted
\item DIA forced photometry persisted
\end{itemize}

{\bf Actions in case of failure:}\\
{\bf Alternative procedures:}\\
\begin{itemize}
\item Butler access to CalExp images from DRP that overlap spatially
  with current CalExp image  for template generation
\end{itemize}
\subsubsection{Key Requirements}

The alert detection pipeline shall difference a visit image against a deeper template, and detect and characterize sources in the difference image in the time required to achieve the 60 second design goal for Level 1 alert processing (current timing allocation: 24 seconds). The algorithms employed by the pipeline shall result in purity and completeness of the sample as required by the \DMSR\@. Image differencing shall perform as well in crowded as in uncrowded fields.

\subsubsection{Baseline Design}
\label{sec:diffimDesign}

\paragraph{Template Generation}~

By either querying for Coadd Images that spatially overlap with a
given CalExp or for a DCR differential chromatic refraction corrected
template (see \hyperref[sec:acRetrieveTemplate]{Retrieve Diffim
  Template}) template image.

\noindent
{\bf Subtasks:}
\begin{itemize}
\item Query for Coadd images that are within a given time interval
  (default 2 years) of the current sensor image, and are within
  allowable airmass limits (default XXX). seexs
  \hyperref[sec:acRetrieveTemplate]{template retrieval}
\item An alternate approach will be to return an interpolated,
  DCR-corrected template based on a model of the effect of DCR (see
  \hyperref[sec:acDCRTemplates]{ DCR template generation}). The
  direction of the DCR correction will be aligned with the 
  ``parallactic angle''.
  \end{itemize}

\paragraph{Image differencing}~

Model the matching kernel and its spatial variation using a regression
approach and a \hyperref[sec:acImageSubtraction]{set of basis functions}

\noindent
{\bf Subtasks:}
\begin{itemize}
\item Match \hyperref[sec:apSourcemeasurement]{DRP} sources and
  sources from \hyperref[sec:apSingleFrameProcessing]{SFP}
\item Determine a relative astrometric solution
\item Warp the template and measurements to the science image frame
\item Determine the appropriate PSF matching sources
\item Decorrelate the science image with an estimate of the science PSF (pre-convolution) 
\item Compute the PSF matching kernel and spatial model using
  \hyperref[sec:acDiffImDecorrelation]{ZOGY} approach
\item Difference the science and template images
\item Apply a correction for correlated noise
\item Difference image \hyperref[sec:acSourceDetection]{source
    detection} to generate DIASources
\item Difference image  \hyperref[sec:acMeasurement]{source
    measurement}: \hyperref[sec:acDipoleModels]{dipole fit}, \hyperref[sec:acTrailedPointSourceModels]{trailed source} measurement
\item Measure flux on snap difference for all DIASources
\end{itemize}

\paragraph{Real-Bogus classification}~

Initial classification step to identify false positives in the image
difference sources. Training of the classifier will be undertaken
outside of the nightly processing and will utilize a training sample
of real variable sources and artifacts based on labelled data. Labels
will be derived from simulated data and visually classified sources.

\noindent
{\bf Subtasks:}
\begin{itemize}
\item The data structures for the real-bogus machine learning
  algorithm(s) will be loaded
\item A random forest or other probabilistic classification algorithm will be
  applied to the DIASources
\item Update the DIASources with the probabilistic classification 
\item Prune the DIASource list based on classifier results
\end{itemize}

\paragraph{Source Association}~

{\bf Input Data:}\\
\begin{itemize}
\item Access to DIAObjects and that overlap spatially with CalExp images
\item Access to Objects that overlap spatially  with CalExp images
\item Access to SSObjects whose \hyperref[sec:acEphemerisCalculation]{ephemerides} overlap spatially with CalExp images
\item Internal reference catalog for CalExp from DRP
\item PSF \hyperref[sec:apSingleFrameProcessing]{measured} from the
  science image
\item Data structures for the real-bogus classifier
\end{itemize}


{\bf Output Data}\\
\begin{itemize}
\item DIAimage persisted
\item DIASources persisted
\item DIAObjects persisted
\item DIA forced photometry persisted
\end{itemize}
\noindent
{\bf Subtasks:}
\begin{itemize}
\item Given the time of an observation and the motion of the sources,
  propagate positions of all sources (SSObjects and DIASources).
\item Using the WCS determine the sensor coordinates for all
  DIAObjects in the catalog
\item Match all DIASources to all DIAObject catalog positions using a probabilistic \hyperref[sec:acDIAObjectGeneration]{matching algorithm}
\item Update associated DIAObjects with aggregate quantities
  e.g. position and flux (in a 30 day rolling wind)
\item Perform forced photometry of all DIAObjects that intersect with
  the frame.
\end{itemize}


\subsubsection{Prototype Implementation}

The prototype code is available at \url{https://github.com/lsst/ip_diffim}. The current prototype, while functional, will require a partial redesign to be transfered to construction to address performance and extensibility concerns.

\clearpage

\subsection{Alert Generation Pipeline (\wbsAP)}

\subsubsection{Key Requirements}

Alert Generation Pipeline shall take the newly discovered \DIASources and all associated metadata as described
in the \DPDD, and deliver alert packets in \VOEvent format to a variety of endpoints via standard IVOA protocols (eg., VOEvent Transport Protocol; VTP\@).

To directly serve the end-users, the Alert Generation Pipeline shall provide a basic, limited capacity, alert filtering service. This service will run at the LSST U.S. Archive Center (at NCSA). It will let astronomers create simple filters that limit what alerts, and what fields from those alerts, are ultimately forwarded to them. These \emph{user defined filters} will be possible to specify using an SQL-like declarative language, or short snippets of (likely Python) code.

Since there is a need to keep both the alert database and the brokers consistent, there is a big win if both the database and the brokers read from the same fault tolerant intermediate persistence format.  A redundant, cluster based, strongly ordered, message system like Kafka (http://kafka.apache.org) is a very attractive option as the intermediate persistence.  It is very similar in concept to persisting to some well known file format with the addition of reduntant storage, configurable expiration time of messages, and strict ordering so database catch-up is trivial.  It is also scalable.

\subsubsection{Baseline Design}

{\bf Input Data:}\\
\begin{itemize}
\item Access to DIAObjects that overlap spatially with CalExp images
\item Access to Template image that overlap spatially with CalExp images
\end{itemize}


{\bf Output Data}\\
\begin{itemize}
\item DIA event table persisted
\item VOEvents
\end{itemize}

\paragraph{Alert generation}~

\noindent
{\bf Subtasks:}
\begin{itemize}
\item Generate postage stamps for all DIASources: direct image and difference image
\item Push alert records to alert persistence
\item Alert database ingestion client reads all new alerts and persists them permanently in the alert database
\end{itemize}

\paragraph{Alert Distribution: To community brokers}
\noindent
{\bf Subtasks:}
\begin{itemize}
\item For each visit read all new alert records from the alert persistence
\item Package each message as a properly formatted VOEvent
\item Bundle VOEvents using a pre-negotiated format
\item Transmit bundled VOEvents to community broker endpoints
\end{itemize}

\paragraph{Alert Distribution: Minimal brokers}
\noindent
Each minimal broker will have some subset of users.  The following is for a single broker.
{\bf Subtasks:}
\begin{itemize}
\item Read new alert records from the alert persistence in order
\item Filter event records for relevance (WHERE)
\item Filter event columns for content (SELECT)
\item Package event as a valid VOEvent
\item Publish the VOEvent to the appropriate endpoint (potentially another messaging queue so clients can access asynchronosly)
\end{itemize}

\paragraph{Forced Photometry on all DIAObjects}~

\noindent
{\bf Subtasks:}
\begin{itemize}
\item Compute forced photometry on all DIAObjects in the field.  This
  does not end up in the alerts.
\item Update the DIA Object forced photometry tables
\end{itemize}

\subsubsection{Prototype Implementation}

\clearpage

\subsection{Precovery Photometry Pipeline}

\noindent
{\bf Input Data:}\\
\begin{itemize}
\item Butler access to DIA images within finite time interval (default 
  30 days) 
\item Butler access to DIAobjects detected from the previous night 
  with no associations 
\end{itemize}
{\bf Output Data}\\
\begin{itemize}
\item Updated and persisted forced photometry tables for all newly
  detected DIAobjects
\end{itemize}

\subsubsection{Key Requirements}

Within 24 hrs.

\paragraph{Precovery of new DIAObjects}~

\noindent
{\bf Subtasks:}
\begin{itemize}
\item Force photometer in difference images for all new DIAObjects for the past 30 days.
\end{itemize}
\clearpage

\subsection{Moving Object Pipeline (\wbsMOPS)}

\subsubsection{Key Requirements}

The Moving Object Pipeline is responsible for generating and managing the Solar System\footnote{Also sometimes referred to as `Moving Object'} data products. These are Solar System objects with associated Keplerian orbits, errors, and detected \DIASources. Quantitatively, it shall be capable of detecting 95\% of all Solar System objects that meet the findability criteria as defined in the \OSS\@. 

\subsubsection{Baseline Design}
{\bf Input Data}\\
\begin{itemize}
\item 'Orphan' DIASources from the last night of observing.  This means DIASources that are not associated with a DIAObject.  DIASources associated with an SSObject in the night are still passed through the MOPS machinery
\item DIAObject database
\item SSObject database
\item Exposure metadata database
\end{itemize}
{\bf Output Data}\\
\begin{itemize}
\item Updated SSObject databaase
\item Updated DIASource database
\end{itemize}
{\bf Anscillary Products}\\
\begin{itemize}
\item Tracklet database
\item Track database
\item Intermediate orbit prediction database
\end{itemize}
{\bf Actions in case of failure}\\
{\bf Alternative procedures}\\

{\bf Subtasks:}
\begin{itemize}
\item Feed all input orphan DIASources to \hyperref[sec:acMakeTracklets][makeTracklets].
\item Run \hyperref[sec:acAttributionAndPrecovery]{attribution and precovery} on with just the tracklets and DIASources from the previous night.  This culls any tracklets or DIASources that obviously belong to an existing SSObject from the rest of the processing.
\item Compute new \hyperref[sec:acOrbitFitting]{orbits} and \hyperref[sec:acOrbitMerging]{merge orbits}.
\item Run \hyperref[sec:acAttributionAndPrecovery]{attribution and precovery} on with full survey of tracklets and DIASources but only running over new SSObjects.
\item \hyperref[sec:acOrbitMerging]{Merge Orbits}.
\end{itemize}

\subsubsection{Prototype Implementation}

Prototype MOPS codes are available at
\url{https://github.com/lsst/mops_daymops} and
\url{https://github.com/lsst/mops_nightmops}. We expect it will be
possible to transfer a significant fraction of the existing code into
Construction. Current DayMOPS prototype already performs within the
computational envelope envisioned for LSST Operations, though it does
not yet reach the required completeness requirement.

